{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2021 Semester 1\n",
    "\n",
    "## Assignment 1: Pose classification with naive Bayes\n",
    "\n",
    "###### Submission deadline: 7 pm, Tuesday 6 Apr 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student ID(s):**     997351\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you will use for your Assignment 1 submission.\n",
    "\n",
    "Marking will be applied on the four functions that are defined in this notebook, and to your responses to the questions at the end of this notebook (Submitted in a separate PDF file).\n",
    "\n",
    "**NOTE: YOU SHOULD ADD YOUR RESULTS, DIAGRAMS AND IMAGES FROM YOUR OBSERVATIONS IN THIS FILE TO YOUR REPORT (the PDF file).**\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find.\n",
    "\n",
    "**Adding proper comments to your code is MANDATORY. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from COMP30027 (Machine Learning, 2021, Semester 2) Assignment 1 by Justin Kelley, 997351, (22/03/2021)\n",
    "# Import required modules.\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import random\n",
    "import copy\n",
    "\n",
    "# Indicates a missing value.\n",
    "MISSING_VALUE = \"9999\"\n",
    "NULL = \"NULL\"\n",
    "NA = None\n",
    "\n",
    "# Penalty for missing probablity such as a class with no instances or missing attribute-class pair.\n",
    "# Cannot have 0 for these cases as log(x) values will be negative for x between 0 and 1.\n",
    "PENALTY = -9999999\n",
    "\n",
    "# Kernel Bandwidths to check.\n",
    "KBs = [5, 7.5, 10, 12.5, 15, 17.5, 20, 22.5, 25]\n",
    "KBs_EXTENEDED = [1, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 50, 100, 200, 500, 1000, 2000]\n",
    "\n",
    "# Different cross validation partition sizes.\n",
    "DEFAULT_M = 10\n",
    "Ms = [1, 2, 5, 10, 20, 50, 100, 200, 400]\n",
    "\n",
    "# Dividers\n",
    "DIVIDER = \"*******************************************************************\"\n",
    "PRIMARY_DIVIDER = \"######################################################################\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractData(filename):\n",
    "    \"\"\"Takes data from file, filename, and splits it into two lists.\n",
    "    One list stores the concept, while the other contains the attributes.\n",
    "    Returns these two lists.\"\"\"\n",
    "    \n",
    "    # List of instances for each attribute.\n",
    "    x = []\n",
    "    \n",
    "    # List of instances for the concept.\n",
    "    y = []\n",
    "    \n",
    "    # Open file and add data to attribute and concept lists.\n",
    "    with open(filename, mode='r') as file:\n",
    "        for line in file:\n",
    "            atts = line.strip().split(\",\")\n",
    "            \n",
    "            # Flag missing values are removable by marking them as null.\n",
    "            for i in range(1, len(atts)):\n",
    "                if atts[i] == MISSING_VALUE:\n",
    "                    atts[i] = NULL\n",
    "                else:\n",
    "                    atts[i] = float(atts[i])\n",
    "            \n",
    "            # Add values to lists.\n",
    "            x.append(atts[1:])\n",
    "            y.append(atts[0])\n",
    "            \n",
    "    return x, y\n",
    "\n",
    "\n",
    "\n",
    "#*************************** PRIMARY FUNCTION START *************************#\n",
    "def preprocess(trainFile, testFile):\n",
    "    \"\"\"Prepares the data by reading in data from a training and testing file\n",
    "    and converts that data into a useful format for training and testing\n",
    "    purposes. Returns the data in this format as seperate test and training sets.\"\"\"\n",
    "    \n",
    "    # Test and training sets attributes.\n",
    "    xTrain = []\n",
    "    xTest = []\n",
    "    \n",
    "    # Test and training sets concepts.\n",
    "    yTrain = []\n",
    "    yTest = []\n",
    "    \n",
    "    # Extract data from test and training files.\n",
    "    xTrain, yTrain = extractData(trainFile)\n",
    "    xTest, yTest = extractData(testFile)\n",
    "    \n",
    "    # Returns testing sets (xTest, yTest), concepts (yTrain) as lists and\n",
    "    # the training attribute set (xTrain) as a dictionary group by class levels.\n",
    "    return xTrain, yTrain, xTest, yTest\n",
    "#**************************** PRIMARY FUNCTION END **************************#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupByClass(xTrain, yTrain):\n",
    "    \"\"\"Takes the attributes in xTrain and groups them by the class levels\n",
    "    stored in yTrain. Returns this grouping as a dictionary that is a list\n",
    "    of lists.\"\"\"\n",
    "    \n",
    "    # Stores attributes by class.\n",
    "    attributesByClass = defaultdict(list)\n",
    "    \n",
    "    # Get each class level of the concept.\n",
    "    for i in range(0, len(yTrain)):\n",
    "        attributesByClass[yTrain[i]] = [[] for i in range(0, len(xTrain[0]))]\n",
    "    \n",
    "    # Obtain the instances for each attribute for each class level.\n",
    "    for i in range(0, len(yTrain)):\n",
    "        # For each instance for every attribute, add it to the appropriate list\n",
    "        # via class level.\n",
    "        features = attributesByClass[yTrain[i]]\n",
    "        for j in range(0, len(features)):\n",
    "            features[j].append(xTrain[i][j])\n",
    "        attributesByClass[yTrain[i]] = features\n",
    "        \n",
    "    return attributesByClass\n",
    "\n",
    "\n",
    "\n",
    "def computeMeanAndStandardDeviation(values):\n",
    "    \"\"\"Takes a list of floats, computes and returns their mean and standard\n",
    "    deviation. Assumes values has length 2 or greater.\"\"\"\n",
    "    \n",
    "    # Compute mean.\n",
    "    mean = sum(values) / len(values)\n",
    "    \n",
    "    # Compute standard deviation.\n",
    "    sd = 0\n",
    "    for value in values:\n",
    "        sd = sd + (value - mean) ** 2\n",
    "    sd = math.sqrt(sd / (len(values) - 1))\n",
    "    \n",
    "    return mean, sd\n",
    "\n",
    "\n",
    "\n",
    "#*************************** PRIMARY FUNCTION START *************************#\n",
    "def train(xTrain, yTrain):\n",
    "    \"\"\"This function calculates the prior probabilities and likelihoods from the training data\n",
    "    and uses it to build a Gaussian naive Bayes model. Returns dictionaries storing\n",
    "    the class priors and likelihoods (mean and standard deviation for each attribute\n",
    "    for each class).\"\"\"\n",
    "    \n",
    "    # Stores the probablity of each class level occuring among the instances.\n",
    "    priors = defaultdict(int)\n",
    "    \n",
    "    # Stores the mean and standard deviation for each attribute for each class\n",
    "    # level which are used to calculate likelihoods using the Gaussian distribution.\n",
    "    likelihoods = defaultdict(list)\n",
    "    \n",
    "    \n",
    "    # Group the attributes in training set by class.\n",
    "    xTrain = groupByClass(xTrain, yTrain)\n",
    "    \n",
    "    # For each dictionary, get all the classes.\n",
    "    for instance in yTrain:\n",
    "        priors[instance] += 1\n",
    "        likelihoods[instance] = []\n",
    "    \n",
    "    \n",
    "    # Compute data for priors and likelihoods.\n",
    "    for classLevel in priors:\n",
    "        # Compute prior probablity for each class (class level).\n",
    "        priors[classLevel] = priors[classLevel] / len(yTrain)\n",
    "        \n",
    "        # Compute mean and standard deviation for each attribute for each\n",
    "        # class (class level).\n",
    "        features = []\n",
    "        for attribute in xTrain[classLevel]:\n",
    "            # Remove missing values.\n",
    "            values = []\n",
    "            for instance in attribute:\n",
    "                if instance != NULL:\n",
    "                    values.append(float(instance))\n",
    "            \n",
    "            # Compute mean and standard deviation. Gaussian distribution needs sd > 0.\n",
    "            if len(values):\n",
    "                mean, sd = computeMeanAndStandardDeviation(values)\n",
    "                if sd == 0.0:\n",
    "                    features.append([NA, NA])\n",
    "                else:\n",
    "                    features.append([mean, sd])\n",
    "            else:\n",
    "                features.append([NA, NA])\n",
    "        \n",
    "        # Add attribute mean and standard deviation data for each class (class level).\n",
    "        likelihoods[classLevel] = features\n",
    "    \n",
    "    return priors, likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GaussianDensity(value, mean, standardDeviation):\n",
    "    \"\"\"Computes the probablity at the point, value, of a Gaussian distribution\n",
    "    with mean, mean, and standard deviation, standardDeviation, using the\n",
    "    density function. Returns this probablity.\"\"\"\n",
    "    \n",
    "    # Compute the expotent for the expotential.\n",
    "    expotent = -0.5 * (((value - mean) / standardDeviation) ** 2)\n",
    "    \n",
    "    # Compute the factor in front of the expotential.\n",
    "    factor = 1 / (standardDeviation * math.sqrt(2 * math.pi))\n",
    "    \n",
    "    # Compute the probablity.\n",
    "    return factor * math.exp(expotent)\n",
    "\n",
    "\n",
    "\n",
    "def GaussianNaiveBayes(likelihoods, classLevel, instance, index):\n",
    "    \"\"\"Computes the likelihood probablity using Gaussian naive bayes.\"\"\"\n",
    "    \n",
    "    # Get parameters for the Gaussian distribution.\n",
    "    value = float(instance[index])\n",
    "    mean = likelihoods[classLevel][index][0]\n",
    "    std = likelihoods[classLevel][index][1]\n",
    "    \n",
    "    # Check if values were computable.\n",
    "    if mean is None or std is None:\n",
    "        return 0\n",
    "    \n",
    "    # Compute and return likelihood probablity through Gaussian distribution.\n",
    "    return GaussianDensity(value, mean, std)\n",
    "\n",
    "\n",
    "\n",
    "#************************ USED FOR QUESTION 3 START *************************#\n",
    "def KDENaiveBayes(likelihoods, classLevel, instance, index, std):\n",
    "    \"\"\"Computes the likelihood probablity using KDE naive bayes.\n",
    "    The standard deviation, std, is the kernel bandwidth.\"\"\"\n",
    "    \n",
    "    # Value from test set.\n",
    "    value = float(instance[index])\n",
    "    \n",
    "    # Use data points from training set as means for Gaussian distributions.\n",
    "    means = []\n",
    "    for mean in likelihoods[classLevel][index]:\n",
    "        # Ignore missing values.\n",
    "        if mean != NULL:\n",
    "            means.append(float(mean))\n",
    "    \n",
    "    # Number of data points.\n",
    "    n = len(means)\n",
    "    if n == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Compute and return kernel density estimate (KDE).\n",
    "    result = 0\n",
    "    for mean in means:\n",
    "        result = result + GaussianDensity(value, mean, std)\n",
    "    return result / n\n",
    "#************************* USED FOR QUESTION 3 END **************************#\n",
    "\n",
    "\n",
    "\n",
    "def getMax(probablities):\n",
    "    \"\"\"For a list of probablities in a dictionary, probablities, for\n",
    "    class levels will find and return the largest one.\"\"\"\n",
    "    \n",
    "    # Class and probablity value for most likely class.\n",
    "    maxClassLevel = None\n",
    "    maxProbablity = None\n",
    "    \n",
    "    # Find highest probablity class.\n",
    "    for classLevel in probablities:\n",
    "        if maxClassLevel is None:\n",
    "            maxClassLevel = classLevel\n",
    "            maxProbablity = probablities[classLevel]\n",
    "        elif maxProbablity < probablities[classLevel]:\n",
    "            maxProbablity = probablities[classLevel]\n",
    "            maxClassLevel = classLevel\n",
    "    \n",
    "    return maxClassLevel\n",
    "\n",
    "\n",
    "\n",
    "#*************************** PRIMARY FUNCTION START *************************#\n",
    "def predict(xTest, yTest, priors, likelihoods, method, kernelBandwidth = 5):\n",
    "    \"\"\"Uses the prior and likelihood probablities to make predictions for the class\n",
    "    level for each instance in xTest. Will either perform Gaussian naive bayes or KDE\n",
    "    naive bayes. Returns a list of the predictions.\"\"\"\n",
    "\n",
    "    # List of predictions made for each instance.\n",
    "    predictions = []\n",
    "    \n",
    "    \n",
    "    # Make a prediction for each instance.\n",
    "    for instance in xTest:\n",
    "        \n",
    "        # For each instance, compute the probablity for each class.\n",
    "        # Use logathirm for calculations to handle floating point issues.\n",
    "        probablities = defaultdict(float)\n",
    "        for classLevel in set(yTest):\n",
    "            \n",
    "            # Get prior probablity. Assign zero posterior probablity for classes with no instances.\n",
    "            if priors[classLevel] == 0:\n",
    "                probablities[classLevel] = PENALTY\n",
    "                break\n",
    "            probablity = math.log(priors[classLevel])\n",
    "            \n",
    "            # Compute and add likelihood probablity to prior probablity.\n",
    "            for i in range(0, len(likelihoods[classLevel])):\n",
    "                if instance[i] != NULL:\n",
    "                    # Compute likelihood using Gaussian naive bayes.  (Basic implamentation prior to questions.)\n",
    "                    if method == \"Gaussian\":\n",
    "                        result = GaussianNaiveBayes(likelihoods, classLevel, instance, i)\n",
    "                    # Compute likelihood using KDE naive bayes. (Question 3 and Question 4.)\n",
    "                    elif method == \"KDE\":\n",
    "                        result = KDENaiveBayes(likelihoods, classLevel, instance, i, kernelBandwidth)\n",
    "                    if result > 0:\n",
    "                        probablity = probablity + math.log(result)\n",
    "                    else:\n",
    "                        probablity = probablity + PENALTY\n",
    "            \n",
    "            probablities[classLevel] = probablity\n",
    "         \n",
    "        # Find class with highest probablity.\n",
    "        predictions.append(getMax(probablities))    \n",
    "    \n",
    "    return predictions\n",
    "#**************************** PRIMARY FUNCTION END **************************#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*************************** PRIMARY FUNCTION START *************************#\n",
    "def evaluate(predictions, yTest):\n",
    "    \"\"\"Computes and returns the accuracy score.\"\"\"\n",
    "    \n",
    "    # Assign zero score if no predictions made.\n",
    "    if len(predictions) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Compute accuracy score.\n",
    "    score = 0\n",
    "    for i in range(0, len(predictions)):\n",
    "        if (predictions[i] == yTest[i]):\n",
    "            score += 1\n",
    "    score = score / len(predictions)\n",
    "    \n",
    "    return score\n",
    "#**************************** PRIMARY FUNCTION END **************************#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#************ SOME OF THE FUNCTIONS USED FOR QUESTION 4 START ***************#\n",
    "def randomCrossValidation(x, y, m, shuffle = True):\n",
    "    \"\"\"Takes the traing data, x and y, shuffles it randomly to ensure greater chance of\n",
    "    class level diversity in each partition for purposes of forming m partitions\n",
    "    for cross-validation. Also allows for no random shuffling.\"\"\"\n",
    "    \n",
    "    # Partitions of attributes and concepts.\n",
    "    xPartitions = []\n",
    "    yPartitions = []\n",
    "    \n",
    "    # All possible pairings of partitions to form training and test sets from m-1 and 1\n",
    "    # partition respectively.\n",
    "    sets = []\n",
    "    \n",
    "    # Partition size. Number of partitions is set to the number of instances.\n",
    "    increment = int(len(y) / m)\n",
    "    \n",
    "    \n",
    "    # Shuffle instances if requested.\n",
    "    if shuffle:\n",
    "        # Group concept together with attributes to ensure consistency when shuffling list.\n",
    "        newList = x\n",
    "        for i in range(0, len(x)):\n",
    "            newList[i].append(y[i])\n",
    "\n",
    "        # Shuffle the list.\n",
    "        random.shuffle(newList)\n",
    "        random.randrange(len(y))\n",
    "\n",
    "        # Seperate list back into attribute and concept compotents.\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in newList:\n",
    "            x.append(i[:-1])\n",
    "            y.append(i[-1])\n",
    "  \n",
    "    \n",
    "    # Form partitions of the data.\n",
    "    last = 0\n",
    "    for i in range(1, m + 1):\n",
    "        if i != m:\n",
    "            xPartitions.append(x[last: i * increment])\n",
    "            yPartitions.append(y[last: i * increment])\n",
    "        else:\n",
    "            xPartitions.append(x[last:])\n",
    "            yPartitions.append(y[last:])\n",
    "        last = i * increment\n",
    "    \n",
    "    \n",
    "    # Join partitions to form different training and test sets.\n",
    "    for i in range(0, len(yPartitions)):\n",
    "        xTrain = []\n",
    "        yTrain = []\n",
    "        \n",
    "        # Test set is made from one partition.\n",
    "        xTest = xPartitions[i]\n",
    "        yTest = yPartitions[i]\n",
    "        \n",
    "        # Training set is made from remaining partitions.\n",
    "        for j in range(0, len(yPartitions)):\n",
    "            if j != i:\n",
    "                xTrain += xPartitions[j]\n",
    "                yTrain += yPartitions[j]\n",
    "        sets.append([xTrain, yTrain, xTest, yTest])\n",
    "    \n",
    "    return sets\n",
    "    \n",
    "    \n",
    "\n",
    "def selectKernelBandwidth(xTrain, yTrain, m, showInfo = False, shuffle = True):\n",
    "    \"\"\"Test the performance of KDE naive bayes with a range of kernel bandwidths using\n",
    "    cross-validation with or without random shuffling of instances for testing in order\n",
    "    to select the best one for the model.\"\"\"\n",
    "    \n",
    "    # Split the training data into several training/test pairs via cross validation.\n",
    "    # Data is shuffled to improve diversity of class levels in each pair.\n",
    "    sets = randomCrossValidation(xTrain, yTrain, m, shuffle)\n",
    "    \n",
    "    \n",
    "    # The kernel bandwidth with the highest score.\n",
    "    maxScore = -1\n",
    "    maxKB = -1\n",
    "    \n",
    "    \n",
    "    # Determine the best kernel bandwidth.\n",
    "    if showInfo:\n",
    "        print(PRIMARY_DIVIDER)\n",
    "        print(f\"Showing accuracy score for each different KB (m = {m}):\")\n",
    "    for KB in KBs:\n",
    "        # Testing sets from each run are to be joined together.\n",
    "        predictions = []\n",
    "        testSet = []\n",
    "        # Run through get combination of test/training pairs formed.\n",
    "        for selection in sets:\n",
    "            # Stores the probablity of each class level occuring among the instances.\n",
    "            priors = defaultdict(int)\n",
    "\n",
    "            # Compute frequency of each class level.\n",
    "            for instance in selection[1]:\n",
    "                priors[instance] += 1\n",
    "\n",
    "            # Compute prior probablity for each class level.\n",
    "            for classLevel in priors:\n",
    "                priors[classLevel] = priors[classLevel] / len(selection[1])\n",
    "            \n",
    "            # Prepare attributes for KDE likelihood estimations.\n",
    "            likelihoods = groupByClass(selection[0], selection[1])\n",
    "            \n",
    "            # Predict class level and add to running list.\n",
    "            predictions += predict(selection[2], selection[3], priors, likelihoods, \"KDE\", KB)\n",
    "            testSet += selection[3]\n",
    "            \n",
    "        # Compute score and check if it is the highest.\n",
    "        score = evaluate(predictions, testSet)\n",
    "        if score > maxScore:\n",
    "            maxScore = score\n",
    "            maxKB = KB\n",
    "        if showInfo:\n",
    "            print(f\"KB = {KB}, has accuracy score of {score}.\")\n",
    "    \n",
    "    \n",
    "    if showInfo:\n",
    "        print(DIVIDER)\n",
    "        print(f\"KB = {maxKB} has highest accuracy score of {maxScore}.\")\n",
    "        print(PRIMARY_DIVIDER)\n",
    "    \n",
    "    return maxKB\n",
    "#************** SOME OF THE FUNCTIONS USED FOR QUESTION 4 END ****************#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################\n",
      "Gaussian naive bayes with default data set has accuracy score 0.716.\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Gaussian naive bayes with default data set has accuracy score 0.716.\n",
      "*******************************************************************\n",
      "KDE naive bayes with default data set and kernel bandwidth of 5 has accuracy score 0.767.\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Gaussian naive bayes with default data set has accuracy score 0.716.\n",
      "*******************************************************************\n",
      "KDE naive bayes with default data set and kernel bandwidth selected by\n",
      "cross validation has accuracy score 0.759.\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Gaussian naive bayes with combined data set has accuracy score 0.737.\n",
      "*******************************************************************\n",
      "KDE naive bayes with combined data set and kernel bandwidth of 5 has accuracy score 0.905.\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Gaussian naive bayes with combined data set has accuracy score 0.737.\n",
      "*******************************************************************\n",
      "KDE naive bayes with combined data set and kernel bandwidth selected by\n",
      "cross validation has accuracy score 0.892.\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Running KDE naive bayes with normal training and testing sets.\n",
      "When KB = 1, accuracy score is 0.664.\n",
      "######################################################################\n",
      "######################################################################\n",
      "When KB = 2, accuracy score is 0.681.\n",
      "######################################################################\n",
      "######################################################################\n",
      "When KB = 4, accuracy score is 0.75.\n",
      "######################################################################\n",
      "######################################################################\n",
      "When KB = 6, accuracy score is 0.767.\n",
      "######################################################################\n",
      "######################################################################\n",
      "When KB = 8, accuracy score is 0.776.\n",
      "######################################################################\n",
      "######################################################################\n",
      "When KB = 10, accuracy score is 0.776.\n",
      "######################################################################\n",
      "######################################################################\n",
      "When KB = 12, accuracy score is 0.767.\n",
      "######################################################################\n",
      "######################################################################\n",
      "When KB = 14, accuracy score is 0.759.\n",
      "######################################################################\n",
      "######################################################################\n",
      "When KB = 16, accuracy score is 0.759.\n",
      "######################################################################\n",
      "######################################################################\n",
      "When KB = 18, accuracy score is 0.741.\n",
      "######################################################################\n",
      "######################################################################\n",
      "When KB = 20, accuracy score is 0.75.\n",
      "######################################################################\n",
      "######################################################################\n",
      "When KB = 22, accuracy score is 0.741.\n",
      "######################################################################\n",
      "######################################################################\n",
      "When KB = 24, accuracy score is 0.733.\n",
      "######################################################################\n",
      "######################################################################\n",
      "When KB = 26, accuracy score is 0.733.\n",
      "######################################################################\n",
      "######################################################################\n",
      "When KB = 28, accuracy score is 0.733.\n",
      "######################################################################\n",
      "######################################################################\n",
      "When KB = 30, accuracy score is 0.707.\n",
      "######################################################################\n",
      "######################################################################\n",
      "When KB = 50, accuracy score is 0.638.\n",
      "######################################################################\n",
      "######################################################################\n",
      "When KB = 100, accuracy score is 0.5.\n",
      "######################################################################\n",
      "######################################################################\n",
      "When KB = 200, accuracy score is 0.276.\n",
      "######################################################################\n",
      "######################################################################\n",
      "When KB = 500, accuracy score is 0.259.\n",
      "######################################################################\n",
      "######################################################################\n",
      "When KB = 1000, accuracy score is 0.259.\n",
      "######################################################################\n",
      "######################################################################\n",
      "When KB = 2000, accuracy score is 0.259.\n",
      "######################################################################\n",
      "######################################################################\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Running KDE naive bayes with normal training and testing sets.\n",
      "Running it with different values for m.\n",
      "######################################################################\n",
      "Showing accuracy score for each different KB (m = 1):\n",
      "KB = 5, has accuracy score of 0.09236947791164658.\n",
      "KB = 7.5, has accuracy score of 0.09236947791164658.\n",
      "KB = 10, has accuracy score of 0.09236947791164658.\n",
      "KB = 12.5, has accuracy score of 0.09236947791164658.\n",
      "KB = 15, has accuracy score of 0.09236947791164658.\n",
      "KB = 17.5, has accuracy score of 0.09236947791164658.\n",
      "KB = 20, has accuracy score of 0.09236947791164658.\n",
      "KB = 22.5, has accuracy score of 0.09236947791164658.\n",
      "KB = 25, has accuracy score of 0.09236947791164658.\n",
      "*******************************************************************\n",
      "KB = 5 has highest accuracy score of 0.09236947791164658.\n",
      "######################################################################\n",
      "When m = 1, KB = 5, accuracy score is 0.767.\n",
      "######################################################################\n",
      "######################################################################\n",
      "######################################################################\n",
      "Showing accuracy score for each different KB (m = 2):\n",
      "KB = 5, has accuracy score of 0.7764390896921017.\n",
      "KB = 7.5, has accuracy score of 0.7911646586345381.\n",
      "KB = 10, has accuracy score of 0.7965194109772423.\n",
      "KB = 12.5, has accuracy score of 0.7965194109772423.\n",
      "KB = 15, has accuracy score of 0.7911646586345381.\n",
      "KB = 17.5, has accuracy score of 0.7871485943775101.\n",
      "KB = 20, has accuracy score of 0.7817938420348058.\n",
      "KB = 22.5, has accuracy score of 0.7697456492637216.\n",
      "KB = 25, has accuracy score of 0.7630522088353414.\n",
      "*******************************************************************\n",
      "KB = 10 has highest accuracy score of 0.7965194109772423.\n",
      "######################################################################\n",
      "When m = 2, KB = 10, accuracy score is 0.776.\n",
      "######################################################################\n",
      "######################################################################\n",
      "######################################################################\n",
      "Showing accuracy score for each different KB (m = 5):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KB = 5, has accuracy score of 0.7951807228915663.\n",
      "KB = 7.5, has accuracy score of 0.8032128514056225.\n",
      "KB = 10, has accuracy score of 0.7991967871485943.\n",
      "KB = 12.5, has accuracy score of 0.7965194109772423.\n",
      "KB = 15, has accuracy score of 0.7925033467202142.\n",
      "KB = 17.5, has accuracy score of 0.7951807228915663.\n",
      "KB = 20, has accuracy score of 0.7938420348058902.\n",
      "KB = 22.5, has accuracy score of 0.7831325301204819.\n",
      "KB = 25, has accuracy score of 0.7777777777777778.\n",
      "*******************************************************************\n",
      "KB = 7.5 has highest accuracy score of 0.8032128514056225.\n",
      "######################################################################\n",
      "When m = 5, KB = 7.5, accuracy score is 0.767.\n",
      "######################################################################\n",
      "######################################################################\n",
      "######################################################################\n",
      "Showing accuracy score for each different KB (m = 10):\n",
      "KB = 5, has accuracy score of 0.8032128514056225.\n",
      "KB = 7.5, has accuracy score of 0.8072289156626506.\n",
      "KB = 10, has accuracy score of 0.7991967871485943.\n",
      "KB = 12.5, has accuracy score of 0.8058902275769746.\n",
      "KB = 15, has accuracy score of 0.7991967871485943.\n",
      "KB = 17.5, has accuracy score of 0.7978580990629184.\n",
      "KB = 20, has accuracy score of 0.7991967871485943.\n",
      "KB = 22.5, has accuracy score of 0.7951807228915663.\n",
      "KB = 25, has accuracy score of 0.785809906291834.\n",
      "*******************************************************************\n",
      "KB = 7.5 has highest accuracy score of 0.8072289156626506.\n",
      "######################################################################\n",
      "When m = 10, KB = 7.5, accuracy score is 0.767.\n",
      "######################################################################\n",
      "######################################################################\n",
      "######################################################################\n",
      "Showing accuracy score for each different KB (m = 20):\n",
      "KB = 5, has accuracy score of 0.8045515394912985.\n",
      "KB = 7.5, has accuracy score of 0.8125836680053548.\n",
      "KB = 10, has accuracy score of 0.8032128514056225.\n",
      "KB = 12.5, has accuracy score of 0.8018741633199464.\n",
      "KB = 15, has accuracy score of 0.7991967871485943.\n",
      "KB = 17.5, has accuracy score of 0.8005354752342704.\n",
      "KB = 20, has accuracy score of 0.8032128514056225.\n",
      "KB = 22.5, has accuracy score of 0.7938420348058902.\n",
      "KB = 25, has accuracy score of 0.785809906291834.\n",
      "*******************************************************************\n",
      "KB = 7.5 has highest accuracy score of 0.8125836680053548.\n",
      "######################################################################\n",
      "When m = 20, KB = 7.5, accuracy score is 0.767.\n",
      "######################################################################\n",
      "######################################################################\n",
      "######################################################################\n",
      "Showing accuracy score for each different KB (m = 50):\n",
      "KB = 5, has accuracy score of 0.8259705488621151.\n",
      "KB = 7.5, has accuracy score of 0.8273092369477911.\n",
      "KB = 10, has accuracy score of 0.8286479250334672.\n",
      "KB = 12.5, has accuracy score of 0.8326639892904953.\n",
      "KB = 15, has accuracy score of 0.8340026773761714.\n",
      "KB = 17.5, has accuracy score of 0.8286479250334672.\n",
      "KB = 20, has accuracy score of 0.8273092369477911.\n",
      "KB = 22.5, has accuracy score of 0.8192771084337349.\n",
      "KB = 25, has accuracy score of 0.8072289156626506.\n",
      "*******************************************************************\n",
      "KB = 15 has highest accuracy score of 0.8340026773761714.\n",
      "######################################################################\n",
      "When m = 50, KB = 15, accuracy score is 0.759.\n",
      "######################################################################\n",
      "######################################################################\n",
      "######################################################################\n",
      "Showing accuracy score for each different KB (m = 100):\n",
      "KB = 5, has accuracy score of 0.8607764390896921.\n",
      "KB = 7.5, has accuracy score of 0.8688085676037484.\n",
      "KB = 10, has accuracy score of 0.8674698795180723.\n",
      "KB = 12.5, has accuracy score of 0.8674698795180723.\n",
      "KB = 15, has accuracy score of 0.8647925033467202.\n",
      "KB = 17.5, has accuracy score of 0.856760374832664.\n",
      "KB = 20, has accuracy score of 0.8554216867469879.\n",
      "KB = 22.5, has accuracy score of 0.8554216867469879.\n",
      "KB = 25, has accuracy score of 0.8487282463186078.\n",
      "*******************************************************************\n",
      "KB = 7.5 has highest accuracy score of 0.8688085676037484.\n",
      "######################################################################\n",
      "When m = 100, KB = 7.5, accuracy score is 0.767.\n",
      "######################################################################\n",
      "######################################################################\n",
      "######################################################################\n",
      "Showing accuracy score for each different KB (m = 200):\n",
      "KB = 5, has accuracy score of 0.8955823293172691.\n",
      "KB = 7.5, has accuracy score of 0.8995983935742972.\n",
      "KB = 10, has accuracy score of 0.8969210174029452.\n",
      "KB = 12.5, has accuracy score of 0.8982597054886211.\n",
      "KB = 15, has accuracy score of 0.892904953145917.\n",
      "KB = 17.5, has accuracy score of 0.891566265060241.\n",
      "KB = 20, has accuracy score of 0.891566265060241.\n",
      "KB = 22.5, has accuracy score of 0.8875502008032129.\n",
      "KB = 25, has accuracy score of 0.8821954484605087.\n",
      "*******************************************************************\n",
      "KB = 7.5 has highest accuracy score of 0.8995983935742972.\n",
      "######################################################################\n",
      "When m = 200, KB = 7.5, accuracy score is 0.767.\n",
      "######################################################################\n",
      "######################################################################\n",
      "######################################################################\n",
      "Showing accuracy score for each different KB (m = 400):\n",
      "KB = 5, has accuracy score of 0.8969210174029452.\n",
      "KB = 7.5, has accuracy score of 0.9022757697456493.\n",
      "KB = 10, has accuracy score of 0.8995983935742972.\n",
      "KB = 12.5, has accuracy score of 0.8982597054886211.\n",
      "KB = 15, has accuracy score of 0.8955823293172691.\n",
      "KB = 17.5, has accuracy score of 0.8982597054886211.\n",
      "KB = 20, has accuracy score of 0.8969210174029452.\n",
      "KB = 22.5, has accuracy score of 0.892904953145917.\n",
      "KB = 25, has accuracy score of 0.8902275769745649.\n",
      "*******************************************************************\n",
      "KB = 7.5 has highest accuracy score of 0.9022757697456493.\n",
      "######################################################################\n",
      "When m = 400, KB = 7.5, accuracy score is 0.767.\n",
      "######################################################################\n",
      "######################################################################\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Running KDE naive bayes with normal training and testing sets.\n",
      "Running it with different values for m.\n",
      "######################################################################\n",
      "Showing accuracy score for each different KB (m = 1):\n",
      "KB = 5, has accuracy score of 0.09236947791164658.\n",
      "KB = 7.5, has accuracy score of 0.09236947791164658.\n",
      "KB = 10, has accuracy score of 0.09236947791164658.\n",
      "KB = 12.5, has accuracy score of 0.09236947791164658.\n",
      "KB = 15, has accuracy score of 0.09236947791164658.\n",
      "KB = 17.5, has accuracy score of 0.09236947791164658.\n",
      "KB = 20, has accuracy score of 0.09236947791164658.\n",
      "KB = 22.5, has accuracy score of 0.09236947791164658.\n",
      "KB = 25, has accuracy score of 0.09236947791164658.\n",
      "*******************************************************************\n",
      "KB = 5 has highest accuracy score of 0.09236947791164658.\n",
      "######################################################################\n",
      "When m = 1, KB = 5, accuracy score is 0.767.\n",
      "######################################################################\n",
      "######################################################################\n",
      "######################################################################\n",
      "Showing accuracy score for each different KB (m = 2):\n",
      "KB = 5, has accuracy score of 0.18741633199464525.\n",
      "KB = 7.5, has accuracy score of 0.18741633199464525.\n",
      "KB = 10, has accuracy score of 0.18741633199464525.\n",
      "KB = 12.5, has accuracy score of 0.18741633199464525.\n",
      "KB = 15, has accuracy score of 0.18741633199464525.\n",
      "KB = 17.5, has accuracy score of 0.18741633199464525.\n",
      "KB = 20, has accuracy score of 0.18741633199464525.\n",
      "KB = 22.5, has accuracy score of 0.18741633199464525.\n",
      "KB = 25, has accuracy score of 0.18741633199464525.\n",
      "*******************************************************************\n",
      "KB = 5 has highest accuracy score of 0.18741633199464525.\n",
      "######################################################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When m = 2, KB = 5, accuracy score is 0.767.\n",
      "######################################################################\n",
      "######################################################################\n",
      "######################################################################\n",
      "Showing accuracy score for each different KB (m = 5):\n",
      "KB = 5, has accuracy score of 0.5488621151271754.\n",
      "KB = 7.5, has accuracy score of 0.5488621151271754.\n",
      "KB = 10, has accuracy score of 0.5488621151271754.\n",
      "KB = 12.5, has accuracy score of 0.5488621151271754.\n",
      "KB = 15, has accuracy score of 0.5488621151271754.\n",
      "KB = 17.5, has accuracy score of 0.5488621151271754.\n",
      "KB = 20, has accuracy score of 0.5488621151271754.\n",
      "KB = 22.5, has accuracy score of 0.5488621151271754.\n",
      "KB = 25, has accuracy score of 0.5488621151271754.\n",
      "*******************************************************************\n",
      "KB = 5 has highest accuracy score of 0.5488621151271754.\n",
      "######################################################################\n",
      "When m = 5, KB = 5, accuracy score is 0.767.\n",
      "######################################################################\n",
      "######################################################################\n",
      "######################################################################\n",
      "Showing accuracy score for each different KB (m = 10):\n",
      "KB = 5, has accuracy score of 0.7322623828647925.\n",
      "KB = 7.5, has accuracy score of 0.7376171352074966.\n",
      "KB = 10, has accuracy score of 0.7402945113788487.\n",
      "KB = 12.5, has accuracy score of 0.7402945113788487.\n",
      "KB = 15, has accuracy score of 0.7402945113788487.\n",
      "KB = 17.5, has accuracy score of 0.7429718875502008.\n",
      "KB = 20, has accuracy score of 0.7429718875502008.\n",
      "KB = 22.5, has accuracy score of 0.7429718875502008.\n",
      "KB = 25, has accuracy score of 0.7416331994645248.\n",
      "*******************************************************************\n",
      "KB = 17.5 has highest accuracy score of 0.7429718875502008.\n",
      "######################################################################\n",
      "When m = 10, KB = 17.5, accuracy score is 0.75.\n",
      "######################################################################\n",
      "######################################################################\n",
      "######################################################################\n",
      "Showing accuracy score for each different KB (m = 20):\n",
      "KB = 5, has accuracy score of 0.9531459170013387.\n",
      "KB = 7.5, has accuracy score of 0.9611780455153949.\n",
      "KB = 10, has accuracy score of 0.9611780455153949.\n",
      "KB = 12.5, has accuracy score of 0.9625167336010709.\n",
      "KB = 15, has accuracy score of 0.963855421686747.\n",
      "KB = 17.5, has accuracy score of 0.9598393574297188.\n",
      "KB = 20, has accuracy score of 0.9611780455153949.\n",
      "KB = 22.5, has accuracy score of 0.9611780455153949.\n",
      "KB = 25, has accuracy score of 0.9598393574297188.\n",
      "*******************************************************************\n",
      "KB = 15 has highest accuracy score of 0.963855421686747.\n",
      "######################################################################\n",
      "When m = 20, KB = 15, accuracy score is 0.759.\n",
      "######################################################################\n",
      "######################################################################\n",
      "######################################################################\n",
      "Showing accuracy score for each different KB (m = 50):\n",
      "KB = 5, has accuracy score of 0.9183400267737617.\n",
      "KB = 7.5, has accuracy score of 0.9183400267737617.\n",
      "KB = 10, has accuracy score of 0.9143239625167336.\n",
      "KB = 12.5, has accuracy score of 0.9129852744310576.\n",
      "KB = 15, has accuracy score of 0.9116465863453815.\n",
      "KB = 17.5, has accuracy score of 0.9129852744310576.\n",
      "KB = 20, has accuracy score of 0.9129852744310576.\n",
      "KB = 22.5, has accuracy score of 0.9129852744310576.\n",
      "KB = 25, has accuracy score of 0.9116465863453815.\n",
      "*******************************************************************\n",
      "KB = 5 has highest accuracy score of 0.9183400267737617.\n",
      "######################################################################\n",
      "When m = 50, KB = 5, accuracy score is 0.767.\n",
      "######################################################################\n",
      "######################################################################\n",
      "######################################################################\n",
      "Showing accuracy score for each different KB (m = 100):\n",
      "KB = 5, has accuracy score of 0.9946452476572959.\n",
      "KB = 7.5, has accuracy score of 0.9946452476572959.\n",
      "KB = 10, has accuracy score of 0.9946452476572959.\n",
      "KB = 12.5, has accuracy score of 0.9946452476572959.\n",
      "KB = 15, has accuracy score of 0.9946452476572959.\n",
      "KB = 17.5, has accuracy score of 0.9946452476572959.\n",
      "KB = 20, has accuracy score of 0.9946452476572959.\n",
      "KB = 22.5, has accuracy score of 0.9946452476572959.\n",
      "KB = 25, has accuracy score of 0.9946452476572959.\n",
      "*******************************************************************\n",
      "KB = 5 has highest accuracy score of 0.9946452476572959.\n",
      "######################################################################\n",
      "When m = 100, KB = 5, accuracy score is 0.767.\n",
      "######################################################################\n",
      "######################################################################\n",
      "######################################################################\n",
      "Showing accuracy score for each different KB (m = 200):\n",
      "KB = 5, has accuracy score of 0.8714859437751004.\n",
      "KB = 7.5, has accuracy score of 0.8714859437751004.\n",
      "KB = 10, has accuracy score of 0.8714859437751004.\n",
      "KB = 12.5, has accuracy score of 0.8714859437751004.\n",
      "KB = 15, has accuracy score of 0.8714859437751004.\n",
      "KB = 17.5, has accuracy score of 0.8714859437751004.\n",
      "KB = 20, has accuracy score of 0.8714859437751004.\n",
      "KB = 22.5, has accuracy score of 0.8714859437751004.\n",
      "KB = 25, has accuracy score of 0.8714859437751004.\n",
      "*******************************************************************\n",
      "KB = 5 has highest accuracy score of 0.8714859437751004.\n",
      "######################################################################\n",
      "When m = 200, KB = 5, accuracy score is 0.767.\n",
      "######################################################################\n",
      "######################################################################\n",
      "######################################################################\n",
      "Showing accuracy score for each different KB (m = 400):\n",
      "KB = 5, has accuracy score of 0.6131191432396251.\n",
      "KB = 7.5, has accuracy score of 0.6131191432396251.\n",
      "KB = 10, has accuracy score of 0.6131191432396251.\n",
      "KB = 12.5, has accuracy score of 0.6131191432396251.\n",
      "KB = 15, has accuracy score of 0.6131191432396251.\n",
      "KB = 17.5, has accuracy score of 0.6131191432396251.\n",
      "KB = 20, has accuracy score of 0.6131191432396251.\n",
      "KB = 22.5, has accuracy score of 0.6131191432396251.\n",
      "KB = 25, has accuracy score of 0.6131191432396251.\n",
      "*******************************************************************\n",
      "KB = 5 has highest accuracy score of 0.6131191432396251.\n",
      "######################################################################\n",
      "When m = 400, KB = 5, accuracy score is 0.767.\n",
      "######################################################################\n",
      "######################################################################\n",
      "######################################################################\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def runBasic():\n",
    "    \"\"\"Runs the Gaussian naive bayes models\n",
    "    using the normal train and testing data sets.\"\"\"\n",
    "    \n",
    "    # Get training and testing data sets from files.\n",
    "    xTrain, yTrain, xTest, yTest = preprocess(\"train.csv\", \"test.csv\")\n",
    "    \n",
    "    # Compute priors and likelihoods.\n",
    "    priors, likelihoods = train(xTrain, yTrain)\n",
    "    \n",
    "    # Run naive bayes using Gaussian method.\n",
    "    predictionsGaussian = predict(xTest, yTest, priors, likelihoods, \"Gaussian\")\n",
    "    \n",
    "    # Compute accuracy score.\n",
    "    scoreGaussian = evaluate(predictionsGaussian, yTest)\n",
    "\n",
    "    # Display results.\n",
    "    print(PRIMARY_DIVIDER)  \n",
    "    print(f\"Gaussian naive bayes with default data set has accuracy score {round(scoreGaussian, 3)}.\")\n",
    "    print(PRIMARY_DIVIDER) \n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    return 0\n",
    "\n",
    "    \n",
    "    \n",
    "def runDefault():\n",
    "    \"\"\"Runs the Gaussian and KDE (with kernal bandwidth of 5) naive bayes models\n",
    "    using the normal train and testing data sets.\"\"\"\n",
    "    \n",
    "    # Get training and testing data sets from files.\n",
    "    xTrain, yTrain, xTest, yTest = preprocess(\"train.csv\", \"test.csv\")\n",
    "    \n",
    "    # Compute priors and likelihoods.\n",
    "    priors, likelihoods = train(xTrain, yTrain)\n",
    "    \n",
    "    # Run naive bayes using Gaussian and KDE with kernel bandwidth of 5.\n",
    "    predictionsGaussian = predict(xTest, yTest, priors, likelihoods, \"Gaussian\")\n",
    "    predictionskDE = predict(xTest, yTest, priors, groupByClass(xTrain, yTrain), \"KDE\")\n",
    "    \n",
    "    # Compute accuracy scores for each method.\n",
    "    scoreGaussian = evaluate(predictionsGaussian, yTest)\n",
    "    scoreKDE = evaluate(predictionskDE, yTest)\n",
    "\n",
    "    # Display results.\n",
    "    print(PRIMARY_DIVIDER)  \n",
    "    print(f\"Gaussian naive bayes with default data set has accuracy score {round(scoreGaussian, 3)}.\")\n",
    "    print(DIVIDER)\n",
    "    print(f\"KDE naive bayes with default data set and kernel bandwidth of 5 has accuracy score {round(scoreKDE, 3)}.\")\n",
    "    print(PRIMARY_DIVIDER)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    return 0\n",
    "    \n",
    "    \n",
    "    \n",
    "def runDefaultWithCrossValidation():\n",
    "    \"\"\"Runs the Gaussian and KDE (with kernal bandwidth selected via cross validation)\n",
    "    naive bayes models using the normal train and testing data sets.\"\"\"\n",
    "    \n",
    "    # Get training and testing data sets from files.\n",
    "    xTrain, yTrain, xTest, yTest = preprocess(\"train.csv\", \"test.csv\")\n",
    "    \n",
    "    # Compute priors and likelihoods.\n",
    "    priors, likelihoods = train(xTrain, yTrain)\n",
    "    \n",
    "    # Compute best kernel bandwidth using randomised cross validation.\n",
    "    KB = selectKernelBandwidth(copy.deepcopy(xTrain), copy.deepcopy(yTrain), DEFAULT_M, shuffle = True)\n",
    "    \n",
    "    # Run naive bayes using Gaussian and KDE with kernel bandwidth of 5.\n",
    "    predictionsGaussian = predict(xTest, yTest, priors, likelihoods, \"Gaussian\")\n",
    "    predictionskDE = predict(xTest, yTest, priors, groupByClass(xTrain, yTrain), \"KDE\", KB)\n",
    "    \n",
    "    # Compute accuracy scores for each method.\n",
    "    scoreGaussian = evaluate(predictionsGaussian, yTest)\n",
    "    scoreKDE = evaluate(predictionskDE, yTest)\n",
    "\n",
    "    # Display results.\n",
    "    print(PRIMARY_DIVIDER)\n",
    "    print(f\"Gaussian naive bayes with default data set has accuracy score {round(scoreGaussian, 3)}.\")\n",
    "    print(DIVIDER)\n",
    "    print(\"KDE naive bayes with default data set and kernel bandwidth selected by\\n\" +\n",
    "          f\"cross validation has accuracy score {round(scoreKDE, 3)}.\")\n",
    "    print(PRIMARY_DIVIDER)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    return 0\n",
    "\n",
    "    \n",
    "    \n",
    "def runUsingWholeDataSet():\n",
    "    \"\"\"Runs the Gaussian and KDE (with kernal bandwidth of 5) naive bayes models\n",
    "    using the whole data set as testing.\"\"\"\n",
    "    \n",
    "    # Get training and testing data sets from files.\n",
    "    xTrain, yTrain, xTest, yTest = preprocess(\"train.csv\", \"combined.csv\")\n",
    "    \n",
    "    # Compute priors and likelihoods.\n",
    "    priors, likelihoods = train(xTrain, yTrain)\n",
    "    \n",
    "    # Run naive bayes using Gaussian and KDE with kernel bandwidth of 5.\n",
    "    predictionsGaussian = predict(xTest, yTest, priors, likelihoods, \"Gaussian\")\n",
    "    predictionskDE = predict(xTest, yTest, priors, groupByClass(xTrain, yTrain), \"KDE\")\n",
    "    \n",
    "    # Compute accuracy scores for each method.\n",
    "    scoreGaussian = evaluate(predictionsGaussian, yTest)\n",
    "    scoreKDE = evaluate(predictionskDE, yTest)\n",
    "\n",
    "    # Display results.\n",
    "    print(PRIMARY_DIVIDER)  \n",
    "    print(f\"Gaussian naive bayes with combined data set has accuracy score {round(scoreGaussian, 3)}.\")\n",
    "    print(DIVIDER)\n",
    "    print(f\"KDE naive bayes with combined data set and kernel bandwidth of 5 has accuracy score {round(scoreKDE, 3)}.\")\n",
    "    print(PRIMARY_DIVIDER)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    return 0\n",
    "    \n",
    "    \n",
    "    \n",
    "def runUsingWholeDataSetWithCrossValidation():\n",
    "    \"\"\"Runs the Gaussian and KDE (with kernal bandwidth selected via cross validation)\n",
    "    naive bayes models using the whole data set as testing.\"\"\"\n",
    "    \n",
    "    # Get training and testing data sets from files.\n",
    "    xTrain, yTrain, xTest, yTest = preprocess(\"train.csv\", \"combined.csv\")\n",
    "    \n",
    "    # Compute priors and likelihoods.\n",
    "    priors, likelihoods = train(xTrain, yTrain)\n",
    "    \n",
    "    # Compute best kernel bandwidth using randomised cross validation.\n",
    "    KB = selectKernelBandwidth(copy.deepcopy(xTrain), copy.deepcopy(yTrain), DEFAULT_M, shuffle = True)\n",
    "    \n",
    "    # Run naive bayes using Gaussian and KDE with kernel bandwidth of 5.\n",
    "    predictionsGaussian = predict(xTest, yTest, priors, likelihoods, \"Gaussian\")\n",
    "    predictionskDE = predict(xTest, yTest, priors, groupByClass(xTrain, yTrain), \"KDE\", KB)\n",
    "    \n",
    "    # Compute accuracy scores for each method.\n",
    "    scoreGaussian = evaluate(predictionsGaussian, yTest)\n",
    "    scoreKDE = evaluate(predictionskDE, yTest)\n",
    "\n",
    "    # Display results.\n",
    "    print(PRIMARY_DIVIDER)\n",
    "    print(f\"Gaussian naive bayes with combined data set has accuracy score {round(scoreGaussian, 3)}.\")\n",
    "    print(DIVIDER)\n",
    "    print(\"KDE naive bayes with combined data set and kernel bandwidth selected by\\n\" +\n",
    "          f\"cross validation has accuracy score {round(scoreKDE, 3)}.\")\n",
    "    print(PRIMARY_DIVIDER)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    return 0\n",
    "    \n",
    "    \n",
    "    \n",
    "def runKDEWithDifferentKBs():\n",
    "    \"\"\"Runs the Gaussian naive bayes model using a range of KBs\n",
    "    using both training and testing data sets.\"\"\"\n",
    "    \n",
    "    # Get training and testing data sets from files.\n",
    "    xTrain, yTrain, xTest, yTest = preprocess(\"train.csv\", \"test.csv\")\n",
    "    \n",
    "    # Compute priors and likelihoods.\n",
    "    priors, likelihoods = train(xTrain, yTrain)\n",
    "    \n",
    "    # Run naive bayes using a range of KBs.\n",
    "    print(PRIMARY_DIVIDER)\n",
    "    print(\"Running KDE naive bayes with normal training and testing sets.\")\n",
    "    for KB in KBs_EXTENEDED:\n",
    "        # Run naive bayes using KDE.\n",
    "        predictionskDE = predict(xTest, yTest, priors, groupByClass(xTrain, yTrain), \"KDE\", KB)\n",
    "\n",
    "        # Compute accuracy scores for each method.\n",
    "        scoreKDE = evaluate(predictionskDE, yTest)\n",
    "\n",
    "        # Display results.\n",
    "        print(f\"When KB = {KB}, accuracy score is {round(scoreKDE, 3)}.\")\n",
    "        print(PRIMARY_DIVIDER)\n",
    "        print(PRIMARY_DIVIDER)\n",
    "    print(PRIMARY_DIVIDER)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "\n",
    "def runKDEWithCrossValidationWithInfo():\n",
    "    \"\"\"Runs the Gaussian naive bayes model using a range of Ms\n",
    "    using both training and testing data sets.\"\"\"\n",
    "    \n",
    "    # Get training and testing data sets from files.\n",
    "    xTrain, yTrain, xTest, yTest = preprocess(\"train.csv\", \"test.csv\")\n",
    "    \n",
    "    # Compute priors and likelihoods.\n",
    "    priors, likelihoods = train(xTrain, yTrain)\n",
    "    \n",
    "    # Run naive bayes using a range of Ms using cross validation.\n",
    "    print(PRIMARY_DIVIDER)\n",
    "    print(\"Running KDE naive bayes with normal training and testing sets.\")\n",
    "    print(\"Running it with different values for m.\")\n",
    "    for m in Ms:\n",
    "        # Compute best kernel bandwidth using randomised cross validation.\n",
    "        KB = selectKernelBandwidth(copy.deepcopy(xTrain), copy.deepcopy(yTrain), m, showInfo = True, shuffle = True)\n",
    "        \n",
    "        # Make predictions using KDE naive bayes.\n",
    "        predictionskDE = predict(xTest, yTest, priors, groupByClass(xTrain, yTrain), \"KDE\", KB)\n",
    "    \n",
    "        # Compute accuracy scores for the method.\n",
    "        scoreKDE = evaluate(predictionskDE, yTest)\n",
    "\n",
    "        # Display results.\n",
    "        print(f\"When m = {m}, KB = {KB}, accuracy score is {round(scoreKDE, 3)}.\")\n",
    "        print(PRIMARY_DIVIDER)\n",
    "        print(PRIMARY_DIVIDER)\n",
    "    print(PRIMARY_DIVIDER)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    return 0\n",
    "    \n",
    "\n",
    "    \n",
    "def runKDECrossValidationWithNoShuffling():\n",
    "    \"\"\"Runs the Gaussian naive bayes model using a range of Ms\n",
    "    using both training and testing data sets without random\n",
    "    shuffling to show the result of this.\"\"\"\n",
    "    \n",
    "    # Get training and testing data sets from files.\n",
    "    xTrain, yTrain, xTest, yTest = preprocess(\"train.csv\", \"test.csv\")\n",
    "    \n",
    "    # Compute priors and likelihoods.\n",
    "    priors, likelihoods = train(xTrain, yTrain)\n",
    "    \n",
    "    # Run naive bayes using a range of Ms using cross validation.\n",
    "    print(PRIMARY_DIVIDER)\n",
    "    print(\"Running KDE naive bayes with normal training and testing sets.\")\n",
    "    print(\"Running it with different values for m.\")\n",
    "    for m in Ms:\n",
    "        # Compute best kernel bandwidth using normal cross validation.\n",
    "        KB = selectKernelBandwidth(copy.deepcopy(xTrain), copy.deepcopy(yTrain), m, showInfo = True, shuffle = False)\n",
    "        \n",
    "        # Make predictions using KDE naive bayes.\n",
    "        predictionskDE = predict(xTest, yTest, priors, groupByClass(xTrain, yTrain), \"KDE\", KB)\n",
    "    \n",
    "        # Compute accuracy scores for the method.\n",
    "        scoreKDE = evaluate(predictionskDE, yTest)\n",
    "\n",
    "        # Display results.\n",
    "        print(f\"When m = {m}, KB = {KB}, accuracy score is {round(scoreKDE, 3)}.\")\n",
    "        print(PRIMARY_DIVIDER)\n",
    "        print(PRIMARY_DIVIDER)\n",
    "    print(PRIMARY_DIVIDER)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    return 0\n",
    "    \n",
    "    \n",
    "    \n",
    "#**************************** DRIVER FUNCTION END ***************************#\n",
    "def runModel():\n",
    "    \"\"\"This functions runs the naive bayes model under different circumstances.\"\"\"\n",
    "    # Each of these functions will run the naive bayes classifer under different\n",
    "    # situations. Comment or uncomment the function to run it. Use the README file\n",
    "    # for further details.\n",
    "    \n",
    "    # Run the Gaussian using the normal train and testing data sets.\n",
    "    runBasic()\n",
    "    \n",
    "    # Run the Gaussian and KDE (with kernal bandwidth of 5) naive bayes models\n",
    "    # using the normal train and testing data sets.\n",
    "    runDefault()\n",
    "    \n",
    "    # Run the Gaussian and KDE (with kernal bandwidth selected via cross validation)\n",
    "    # naive bayes models using the normal train and testing data sets.\n",
    "    runDefaultWithCrossValidation()\n",
    "    \n",
    "    # Run the Gaussian and KDE (with kernal bandwidth of 5) naive bayes models\n",
    "    # using the whole data set as testing.\n",
    "    runUsingWholeDataSet()\n",
    "    \n",
    "    # Run the Gaussian and KDE (with kernal bandwidth selected via cross validation)\n",
    "    # naive bayes models using the whole data set as testing.\"\"\"\n",
    "    runUsingWholeDataSetWithCrossValidation()\n",
    "    \n",
    "    # Run the Gaussian naive bayes model using a range of KBs\n",
    "    # using both training and testing data sets.\n",
    "    runKDEWithDifferentKBs()\n",
    "    \n",
    "    # Run the Gaussian naive bayes model using a range of Ms\n",
    "    # using both training and testing data sets.\n",
    "    runKDEWithCrossValidationWithInfo()\n",
    "    \n",
    "    # Run the Gaussian naive bayes model using a range of Ms\n",
    "    # using both training and testing data sets without random\n",
    "    # shuffling to demonstrate the consquences.\n",
    "    runKDECrossValidationWithNoShuffling()\n",
    "    \n",
    "    return 0\n",
    "#**************************** DRIVER FUNCTION END ***************************#\n",
    "    \n",
    "runModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions \n",
    "\n",
    "\n",
    "If you are in a group of 1, you will respond to **two** questions of your choosing.\n",
    "\n",
    "If you are in a group of 2, you will respond to **four** questions of your choosing.\n",
    "\n",
    "A response to a question should take about 100250 words, and make reference to the data wherever possible.\n",
    "\n",
    "#### NOTE: you may develope codes or functions to help respond to the question here, but your formal answer should be submitted separately as a PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1\n",
    "Since this is a multiclass classification problem, there are multiple ways to compute precision, recall, and F-score for this classifier. Implement at least two of the methods from the \"Model Evaluation\" lecture and discuss any differences between them. (The implementation should be your own and should not just call a pre-existing function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2\n",
    "The Gaussian nave Bayes classifier assumes that numeric attributes come from a Gaussian distribution. Is this assumption always true for the numeric attributes in this dataset? Identify some cases where the Gaussian assumption is violated and describe any evidence (or lack thereof) that this has some effect on the classifiers predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3\n",
    "Implement a kernel density estimate (KDE) naive Bayes classifier and compare its performance to the Gaussian naive Bayes classifier. Recall that KDE has kernel bandwidth as a free parameter -- you can choose an arbitrary value for this, but a value in the range 5-25 is recommended. Discuss any differences you observe between the Gaussian and KDE naive Bayes classifiers. (As with the Gaussian naive Bayes, this KDE naive Bayes implementation should be your own and should not just call a pre-existing function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4\n",
    "Instead of using an arbitrary kernel bandwidth for the KDE naive Bayes classifier, use random hold-out or cross-validation to choose the kernel bandwidth. Discuss how this changes the model performance compared to using an arbitrary kernel bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5\n",
    "Naive Bayes ignores missing values, but in pose recognition tasks the missing values can be informative. Missing values indicate that some part of the body was obscured and sometimes this is relevant to the pose (e.g., holding one hand behind the back). Are missing values useful for this task? Implement a method that incorporates information about missing values and demonstrate whether it changes the classification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6\n",
    "Engineer your own pose features from the provided keypoints. Instead of using the (x,y) positions of keypoints, you might consider the angles of the limbs or body, or the distances between pairs of keypoints. How does a naive Bayes classifier based on your engineered features compare to the classifier using (x,y) values? Please note that we are interested in explainable features for pose recognition, so simply putting the (x,y) values in a neural network or similar to get an arbitrary embedding will not receive full credit for this question. You should be able to explain the rationale behind your proposed features. Also, don't forget the conditional independence assumption of naive Bayes when proposing new features -- a large set of highly-correlated features may not work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
